{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.spaces.utils import flatdim\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import trange\n",
    "from tqdm.auto import tqdm  # notebook compatible\n",
    "from minigrid.envs import FourRoomsEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Transition node. Represents the child of a node as a result of an action being taken. \n",
    "    \"\"\"\n",
    "    def __init__(self, state, action=None, prior=0):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "        self.parent = None\n",
    "        self.children = {}\n",
    "\n",
    "        self.value = 0.0\n",
    "        self.visits = 0\n",
    "        self.prior = prior\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Node: (s:{self.state}, a:{self.action}, r:{self.reward}, d:{self.done}, p:{self.prior})\"\n",
    "    \n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def update_stats(self, val):\n",
    "        self.visits += 1\n",
    "        self.value += (val - self.value) / float(self.visits)\n",
    "\n",
    "class MCTS_AlphaZero:\n",
    "    \"\"\"\n",
    "    MCTS using a similar approach to AlphaZero: https://arxiv.org/pdf/1712.01815.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, cur_state, model, network, hparams):\n",
    "        self.cur_state = cur_state\n",
    "        self.model = model\n",
    "        self.network = network\n",
    "        self.iters: int = hparams[\"search_iters\"]\n",
    "        self.discount = hparams.get(\"discount\", 0.99) \n",
    "        self.c_puct = 1\n",
    "    \n",
    "    def search(self):\n",
    "        root = Node(self.cur_state)\n",
    "        self._expand(root)\n",
    "        #root.prior += dirichlet_noise()\n",
    "        for _ in range(self.iters):\n",
    "            # Tree Policy\n",
    "            # Run through the tree and recursively select the best nodes with respect to their `PUCT` values\n",
    "            next_node = root\n",
    "            while next_node.is_expanded():\n",
    "                next_node = self._PUCT(next_node)\n",
    "\n",
    "            # Expand the leaf node by evaluating network for policy probs and value at state, sample most likely action\n",
    "            value = self._expand(next_node)\n",
    "\n",
    "            # Backup the value of this node or the reward if its a terminal state (TODO: Seems suspicious for non terminal rewards)\n",
    "            self._backup(next_node, value)\n",
    "        \n",
    "        return self._best_action(root), root\n",
    "\n",
    "    # \"Most Robust Child\" selection: http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf\n",
    "    def _best_action(self, root):\n",
    "        return max(root.children.values(), key = lambda child: child.visits).action\n",
    "\n",
    "    def _expand(self, node: Node) -> Node:\n",
    "\n",
    "        # Expand and add children with predicted prior\n",
    "        prior, value = self.network(torch.Tensor([node.state]))\n",
    "        prior, value = prior.detach().numpy(), value.detach().numpy()[0]\n",
    "\n",
    "        prior = self._normalize(prior)\n",
    "        for action in self.model.actions(node.state):\n",
    "            next_obs, r, done, _ = self.model.step(node.state, action)\n",
    "\n",
    "            # Update tree with transition\n",
    "            next_node = Node(next_obs, action=action, prior=prior[action])\n",
    "            next_node.parent = node\n",
    "            next_node.reward = r\n",
    "            next_node.done = done\n",
    "            node.children[action] = next_node\n",
    "\n",
    "        return value\n",
    "\n",
    "\n",
    "    # Detailed here: https://web.stanford.edu/~surag/posts/alphazero.html\n",
    "    def _PUCT(self, node: Node) -> Node:\n",
    "\n",
    "        # Get children and compute state-action values\n",
    "        children: list[Node] = list(node.children.values())\n",
    "        q_vals = np.array([child.value for child in children])\n",
    "\n",
    "        # PUCT takes into account model probs + visitation counts\n",
    "        puct_vals = q_vals +  self.c_puct * np.array([node.children[a].prior * np.sqrt(node.visits / (1 + node.children[a].visits)) for a in node.children.keys()])\n",
    "        \n",
    "        if (np.any(puct_vals)):\n",
    "            puct_vals = self._normalize(puct_vals)\n",
    "            return np.random.choice(children, p=puct_vals)\n",
    "        \n",
    "        return np.random.choice(children)\n",
    "\n",
    "\n",
    "\n",
    "    def _backup(self, node: Node, value: float) -> None:\n",
    "        node.update_stats(value)\n",
    "\n",
    "        while node.parent:\n",
    "            node = node.parent\n",
    "            node.update_stats(self.discount * value)\n",
    "    \n",
    "    def _normalize(self, arr):\n",
    "        shifted = (arr - np.min(arr))\n",
    "        return shifted /np.sum(shifted)\n",
    "\n",
    "\n",
    "def episode(env, model, network, memory, config):\n",
    "    # Run network in evaluation mode\n",
    "    network.eval()\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    rewards = []\n",
    "    logits = []\n",
    "    ep_obs = []\n",
    "    while not done:\n",
    "        action, root = MCTS_AlphaZero(obs, model, network, config).search()\n",
    "        obs, r, done, _ = env.step(action)\n",
    "\n",
    "        rewards.append(r)\n",
    "        ep_obs.append(obs)\n",
    "        logits.append([child.prior for child in root.children.values()])\n",
    "\n",
    "    returns = compute_returns(rewards, config[\"discount\"])\n",
    "\n",
    "    for i in range(len(returns)):\n",
    "        memory.store_transition(ep_obs[i], logits[i], returns[i])\n",
    "\n",
    "# Compute discounts\n",
    "# https://stackoverflow.com/questions/47970683/vectorize-a-numpy-discount-calculation\n",
    "def compute_returns(rewards, discount):\n",
    "    \"\"\"\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = scipy.signal.lfilter(b, a, x=r)\n",
    "    return y[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Models\n",
    "class FrozenLakeModel:\n",
    "    def __init__(self, transitions):\n",
    "        self.model = transitions\n",
    "\n",
    "    def step(self, obs, action):\n",
    "        _, next_obs, r, done =  self.model[obs][action][0]\n",
    "        return next_obs, r, done, _\n",
    "    \n",
    "    def actions(self, obs):\n",
    "        return list(self.model[obs].keys())\n",
    "\n",
    "class FourRoomsModel:\n",
    "    def __init__(self, goal_pos=(16,16), seed=42):\n",
    "        self.goal_pos = goal_pos\n",
    "        self.seed = seed\n",
    "\n",
    "    def step(self, agent_state, action) -> tuple[_, float]:\n",
    "        agent_pos, agent_dir = agent_state\n",
    "        env = FourRoomsEnvPos(agent_pos=agent_pos, goal_pos=self.goal_pos)\n",
    "        env.reset(seed=self.seed)\n",
    "        env.agent_dir = agent_dir\n",
    "        return env.step(action)\n",
    "\n",
    "    def actions(self, obs) -> list[int]:\n",
    "        return list(range(env.action_space.n - 4))\n",
    "\n",
    "class FourRoomsEnvPos(FourRoomsEnv):\n",
    "    def __init__(self, agent_pos=None, goal_pos=None, **kwargs):\n",
    "        super().__init__(agent_pos=agent_pos, goal_pos=goal_pos, **kwargs)\n",
    "        self.max_steps = 1000\n",
    "    \n",
    "    def step(self, action):\n",
    "        _, r, terminated, truncated, _ = super().step(action)\n",
    "        return (self.agent_pos, self.agent_dir), int(self.agent_pos == self._goal_default_pos), terminated or truncated, _\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        obs, _ = super().reset(seed=seed)\n",
    "        return self.agent_pos, self.agent_dir\n",
    "\n",
    "lake_actions = {\n",
    "    0: \"LEFT\",\n",
    "    1: \"DOWN\", \n",
    "    2: \"RIGHT\",\n",
    "    3: \"UP\"\n",
    "}\n",
    "\n",
    "room_actions = {\n",
    "    0: \"left\",\n",
    "    1: \"right\",\n",
    "    2: \"forward\",\n",
    "    3: \"pickup\",\n",
    "    4: \"drop\",\n",
    "    5: \"toggle\",\n",
    "    6: \"done\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Tools to train model\n",
    "class DenseMCTSNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Linear(84, env.action_space.n) \n",
    "        self.value_head = nn.Linear(84, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return self.policy_head(x), self.value_head(x)\n",
    "\n",
    "# Replay memory to store transitions, in a cyclical buffer so olded transitions are removed first (Could be interesting to try a prioritized buffer instead)\n",
    "class ReplayMemory():\n",
    "    def __init__(self, size, env):\n",
    "        self.size = size\n",
    "        self.counter = 0\n",
    "\n",
    "        obs_shape = env.observation_space.shape\n",
    "\n",
    "        self.obs = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.action_probs = np.zeros((size, env.action_space.n), env.action_space.dtype)\n",
    "        self.returns = np.zeros((size, 1), dtype=np.float32)\n",
    "    \n",
    "    def store_transition(self, obs, action_prob, r):\n",
    "        indx = self.counter % self.size\n",
    "\n",
    "        self.obs[indx] = np.array(obs).copy()\n",
    "        self.action_probs[indx] = np.array(action_prob).copy()\n",
    "        self.returns[indx] = np.array(r).copy()\n",
    "\n",
    "        self.counter += 1\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        obs = torch.as_tensor(self.obs[batch])\n",
    "        action_probs = torch.as_tensor(self.action_probs[batch])\n",
    "        returns = torch.as_tensor(self.returns[batch])\n",
    "\n",
    "        return obs, action_probs, returns\n",
    "\n",
    "\n",
    "def train_on_batch(network, optimizer, batch):\n",
    "    # Run network in training mode\n",
    "    network.train()\n",
    "\n",
    "    obs = batch[0]\n",
    "    action_probs = batch[1]\n",
    "    returns = batch[2]\n",
    "\n",
    "    policy_probs, pred_vals = network(obs)\n",
    "\n",
    "    value_loss = nn.MSELoss()(pred_vals, returns)\n",
    "    policy_loss = nn.CrossEntropyLoss()(action_probs, policy_probs)\n",
    "\n",
    "    loss = value_loss + policy_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def policy_iteration(env, model, network, config):\n",
    "    memory = ReplayMemory(config[\"memory_size\"], env)\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    for global_step in tqdm(range(config[\"training_steps\"])):\n",
    "        episode(env, model, network, memory, config)\n",
    "\n",
    "        # Only train once buffer large enough\n",
    "        if (memory.counter > memory.size and global_step % config[\"train_freq\"] == 0):\n",
    "            batch = memory.sample_batch(config[\"batch_size\"])\n",
    "            network = train_on_batch(network, optimizer, batch) \n",
    "    return network\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    \"training_steps\": 250_000,\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"memory_size\": 50_000,\n",
    "    \"discount\": 0.99,\n",
    "    \"train_freq\": 4,\n",
    "    \"search_iters\": 800\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 203/250000 [08:44<179:22:58,  2.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m FrozenLakeModel(env\u001b[39m.\u001b[39mP)\n\u001b[1;32m      3\u001b[0m net \u001b[39m=\u001b[39m DenseMCTSNet(env)\n\u001b[0;32m----> 5\u001b[0m policy_iteration(env, model, net, DEFAULT_CONFIG)\n",
      "Cell \u001b[0;32mIn [4], line 77\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[0;34m(env, model, network, config)\u001b[0m\n\u001b[1;32m     75\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(network\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m global_step \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(config[\u001b[39m\"\u001b[39m\u001b[39mtraining_steps\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[0;32m---> 77\u001b[0m     episode(env, model, network, memory, config)\n\u001b[1;32m     79\u001b[0m     \u001b[39m# Only train once buffer large enough\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m (memory\u001b[39m.\u001b[39mcounter \u001b[39m>\u001b[39m memory\u001b[39m.\u001b[39msize \u001b[39mand\u001b[39;00m global_step \u001b[39m%\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mtrain_freq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "Cell \u001b[0;32mIn [2], line 124\u001b[0m, in \u001b[0;36mepisode\u001b[0;34m(env, model, network, memory, config)\u001b[0m\n\u001b[1;32m    122\u001b[0m ep_obs \u001b[39m=\u001b[39m []\n\u001b[1;32m    123\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m--> 124\u001b[0m     action, root \u001b[39m=\u001b[39m MCTS_AlphaZero(obs, model, network, config)\u001b[39m.\u001b[39;49msearch()\n\u001b[1;32m    125\u001b[0m     obs, r, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    127\u001b[0m     rewards\u001b[39m.\u001b[39mappend(r)\n",
      "Cell \u001b[0;32mIn [2], line 52\u001b[0m, in \u001b[0;36mMCTS_AlphaZero.search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     next_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_PUCT(next_node)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Expand the leaf node by evaluating network for policy probs and value at state, sample most likely action\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_expand(next_node)\n\u001b[1;32m     54\u001b[0m \u001b[39m# Backup the value of this node or the reward if its a terminal state (TODO: Seems suspicious for non terminal rewards)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backup(next_node, value)\n",
      "Cell \u001b[0;32mIn [2], line 66\u001b[0m, in \u001b[0;36mMCTS_AlphaZero._expand\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_expand\u001b[39m(\u001b[39mself\u001b[39m, node: Node) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Node:\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m     \u001b[39m# Expand and add children with predicted prior\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     prior, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(torch\u001b[39m.\u001b[39;49mTensor([node\u001b[39m.\u001b[39;49mstate]))\n\u001b[1;32m     67\u001b[0m     prior, value \u001b[39m=\u001b[39m prior\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), value\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m     prior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [4], line 19\u001b[0m, in \u001b[0;36mDenseMCTSNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(x)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_head(x), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_head(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "model = FrozenLakeModel(env.P)\n",
    "net = DenseMCTSNet(env)\n",
    "\n",
    "policy_iteration(env, model, net, DEFAULT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
