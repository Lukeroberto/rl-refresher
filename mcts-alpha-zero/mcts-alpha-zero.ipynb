{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.spaces.utils import flatdim\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import trange\n",
    "from tqdm.auto import tqdm  # notebook compatible\n",
    "from minigrid.envs import FourRoomsEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Transition node. Represents the child of a node as a result of an action being taken. \n",
    "    \"\"\"\n",
    "    def __init__(self, state, action=None, prior=0):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "\n",
    "        self.parent = None\n",
    "        self.children = {}\n",
    "\n",
    "        self.value = 0.0\n",
    "        self.visits = 0\n",
    "        self.prior = prior\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Node: (s:{self.state}, a:{self.action}, r:{self.reward}, d:{self.done}, p:{self.prior})\"\n",
    "    \n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def update_stats(self, val):\n",
    "        self.visits += 1\n",
    "        self.value += (val - self.value) / float(self.visits)\n",
    "\n",
    "class MCTS_AlphaZero:\n",
    "    \"\"\"\n",
    "    MCTS using a similar approach to AlphaZero: https://arxiv.org/pdf/1712.01815.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, cur_state, model, network, hparams):\n",
    "        self.cur_state = cur_state\n",
    "        self.model = model\n",
    "        self.network = network\n",
    "        self.iters: int = hparams[\"iters\"]\n",
    "        self.discount = hparams.get(\"discount\", 0.99) \n",
    "        self.sim_depth = hparams.get(\"sim_depth\", 1000)\n",
    "        self.c_puct = 1\n",
    "    \n",
    "    def search(self):\n",
    "        root = Node(self.cur_state)\n",
    "        self._expand(root)\n",
    "        #root.prior += dirichlet_noise()\n",
    "        for _ in range(self.iters):\n",
    "            # Tree Policy\n",
    "            # Run through the tree and recursively select the best nodes with respect to their `PUCT` values\n",
    "            next_node = root\n",
    "            while next_node.is_expanded():\n",
    "                next_node = self._PUCT(next_node)\n",
    "\n",
    "            # Expand the leaf node by evaluating network for policy probs and value at state, sample most likely action\n",
    "            value = self._expand(next_node)\n",
    "\n",
    "            # Backup the value of this node or the reward if its a terminal state (TODO: Seems suspicious for non terminal rewards)\n",
    "            self._backup(next_node, value)\n",
    "        \n",
    "        return self._best_action(root), root\n",
    "\n",
    "    # \"Most Robust Child\" selection: http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf\n",
    "    def _best_action(self, root):\n",
    "        return max(root.children.values(), key = lambda child: child.visits).action\n",
    "\n",
    "    def _expand(self, node: Node) -> Node:\n",
    "\n",
    "        # Expand and add children with predicted prior\n",
    "        prior, value = self.network(node.state)\n",
    "        prior /= np.sum(prior)\n",
    "\n",
    "        for action in self.model.actions(node.state):\n",
    "            next_obs, r, done, _ = self.model.step(node.state, action)\n",
    "\n",
    "            # Update tree with transition\n",
    "            next_node = Node(next_obs, action=action, prior=prior[action])\n",
    "            next_node.parent = node\n",
    "            next_node.reward = r\n",
    "            next_node.done = done\n",
    "            node.children[action] = next_node\n",
    "\n",
    "        return value\n",
    "\n",
    "\n",
    "    # Detailed here: https://web.stanford.edu/~surag/posts/alphazero.html\n",
    "    def _PUCT(self, node: Node) -> Node:\n",
    "\n",
    "        # Get children and compute state-action values\n",
    "        children: list[Node] = list(node.children.values())\n",
    "        q_vals = np.array([child.value for child in children])\n",
    "\n",
    "        # PUCT takes into account model probs + visitation counts\n",
    "        puct_vals = q_vals +  self.c_puct * np.array([node.children[a].prior * np.sqrt(node.visits / (1 + node.children[a].visits)) for a in node.children.keys()])\n",
    "        return np.random.choice(children, p=puct_vals/sum(puct_vals)) if np.any(puct_vals) else np.random.choice(children)\n",
    "\n",
    "\n",
    "\n",
    "    def _backup(self, node: Node, value: float) -> None:\n",
    "        node.update_stats(value)\n",
    "\n",
    "        while node.parent:\n",
    "            node = node.parent\n",
    "            node.update_stats(self.discount * value)\n",
    "\n",
    "def episode(env, model, network, config):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    states = [obs]\n",
    "    logits = []\n",
    "    while not done:\n",
    "        action, root = MCTS_AlphaZero(obs, model, network, config).search()\n",
    "        obs, r, done, _ = env.step(action)\n",
    "\n",
    "        rewards.append(r)\n",
    "        obs.append(obs)\n",
    "        logits.append([child.prior for child in root.children.values()])\n",
    "\n",
    "    returns = compute_returns(rewards, config[\"discount\"])\n",
    "\n",
    "    return states, logits, returns\n",
    "\n",
    "\n",
    "\n",
    "# Compute discounts\n",
    "# https://stackoverflow.com/questions/47970683/vectorize-a-numpy-discount-calculation\n",
    "def compute_returns(rewards, discount):\n",
    "    \"\"\"\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = scipy.signal.lfilter(b, a, x=r)\n",
    "    return y[::-1]\n",
    "\n",
    "# For viz purposes\n",
    "def unpack(root: Node, graph, ignore, action_map=None):\n",
    "    if (not root.parent):\n",
    "        graph.node(str(root.state))\n",
    "\n",
    "    if len(root.children.values()) == 0:\n",
    "        return graph\n",
    "    \n",
    "    for child in root.children.values():\n",
    "        if (not (str(root.state), str(child.state), child.action) in ignore):\n",
    "            graph.edge(str(root.state), str(child.state), label=str(child.action if not action_map else action_map[child.action]))\n",
    "            ignore.add((str(root.state), str(child.state), child.action))\n",
    "\n",
    "        graph = unpack(child, graph, ignore, action_map=action_map)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Visualize visitation frequency\n",
    "def unpack_visits(root: Node, visit_freqs=defaultdict(lambda: 0)):\n",
    "    if len(root.children.values()) == 0:\n",
    "        visit_freqs[root.state] += root.visits\n",
    "    \n",
    "    for child in root.children.values():\n",
    "        visit_freqs = unpack_visits(child, visit_freqs)\n",
    "    \n",
    "    return visit_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Models\n",
    "class FrozenLakeModel:\n",
    "    def __init__(self, transitions):\n",
    "        self.model = transitions\n",
    "\n",
    "    def step(self, obs, action):\n",
    "        _, next_obs, r, done =  self.model[obs][action][0]\n",
    "        return next_obs, r, done, _\n",
    "    \n",
    "    def actions(self, obs):\n",
    "        return list(self.model[obs].keys())\n",
    "\n",
    "class FourRoomsModel:\n",
    "    def __init__(self, goal_pos=(16,16), seed=42):\n",
    "        self.goal_pos = goal_pos\n",
    "        self.seed = seed\n",
    "\n",
    "    def step(self, agent_state, action) -> tuple[_, float]:\n",
    "        agent_pos, agent_dir = agent_state\n",
    "        env = FourRoomsEnvPos(agent_pos=agent_pos, goal_pos=self.goal_pos)\n",
    "        env.reset(seed=self.seed)\n",
    "        env.agent_dir = agent_dir\n",
    "        return env.step(action)\n",
    "\n",
    "    def actions(self, obs) -> list[int]:\n",
    "        return list(range(env.action_space.n - 4))\n",
    "\n",
    "class FourRoomsEnvPos(FourRoomsEnv):\n",
    "    def __init__(self, agent_pos=None, goal_pos=None, **kwargs):\n",
    "        super().__init__(agent_pos=agent_pos, goal_pos=goal_pos, **kwargs)\n",
    "        self.max_steps = 1000\n",
    "    \n",
    "    def step(self, action):\n",
    "        _, r, terminated, truncated, _ = super().step(action)\n",
    "        return (self.agent_pos, self.agent_dir), int(self.agent_pos == self._goal_default_pos), terminated or truncated, _\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        obs, _ = super().reset(seed=seed)\n",
    "        return self.agent_pos, self.agent_dir\n",
    "\n",
    "lake_actions = {\n",
    "    0: \"LEFT\",\n",
    "    1: \"DOWN\", \n",
    "    2: \"RIGHT\",\n",
    "    3: \"UP\"\n",
    "}\n",
    "\n",
    "room_actions = {\n",
    "    0: \"left\",\n",
    "    1: \"right\",\n",
    "    2: \"forward\",\n",
    "    3: \"pickup\",\n",
    "    4: \"drop\",\n",
    "    5: \"toggle\",\n",
    "    6: \"done\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LEFT', 0.6662871439999686), ('DOWN', 0.6662886406757791), ('RIGHT', 0.6662873477884159), ('UP', 0.6662888686228624)]\n",
      "Moving LEFT\n",
      "[('LEFT', 0.9646975123523664), ('DOWN', 0.9646967659579148), ('RIGHT', 0.9646973607226275), ('UP', 0.964700462357331)]\n",
      "Moving DOWN\n",
      "[('LEFT', 0.13936913354720237), ('DOWN', 0.13936945684088048), ('RIGHT', 0.13936948138260022), ('UP', 0.13936886370336188)]\n",
      "Moving UP\n",
      "[('LEFT', 0.7474842951844973), ('DOWN', 0.7474850200932024), ('RIGHT', 0.7474831605048385), ('UP', 0.7474863098393514)]\n",
      "Moving RIGHT\n",
      "[('LEFT', 0.509061998711699), ('DOWN', 0.5090631362076741), ('RIGHT', 0.5090624218659394), ('UP', 0.5090603223338668)]\n",
      "Moving UP\n",
      "[('LEFT', 0.48741800733446816), ('DOWN', 0.48741889185075565), ('RIGHT', 0.48741698699358216), ('UP', 0.48741923376461466)]\n",
      "Moving RIGHT\n",
      "[('LEFT', 0.5445073110211908), ('DOWN', 0.5445052745593671), ('RIGHT', 0.544505445057249), ('UP', 0.5445049414631195)]\n",
      "Moving UP\n",
      "[('LEFT', 0.1710495733766843), ('DOWN', 0.17104994327389064), ('RIGHT', 0.1710500652869196), ('UP', 0.17104911979850337)]\n",
      "Moving UP\n",
      "[('LEFT', 0.7434813773359124), ('DOWN', 0.7434839564416055), ('RIGHT', 0.7434814905929161), ('UP', 0.743482551007282)]\n",
      "Moving LEFT\n",
      "[('LEFT', 0.8373763393901617), ('DOWN', 0.8373782662010892), ('RIGHT', 0.8373759537079078), ('UP', 0.8373775528782283)]\n",
      "Moving RIGHT\n",
      "[('LEFT', 0.963275075305788), ('DOWN', 0.9632753745696709), ('RIGHT', 0.9632769444785649), ('UP', 0.9632766202859965)]\n",
      "Moving LEFT\n",
      "[('LEFT', 0.39031948017569984), ('DOWN', 0.3903205321831339), ('RIGHT', 0.3903191191828014), ('UP', 0.3903196043419159)]\n",
      "Moving RIGHT\n",
      "[('LEFT', 0.28748933718580577), ('DOWN', 0.28749116762257215), ('RIGHT', 0.28749064156756127), ('UP', 0.28748966806346077)]\n",
      "Moving LEFT\n",
      "[('LEFT', 0.947065438783874), ('DOWN', 0.9470594088824565), ('RIGHT', 0.9470616577967379), ('UP', 0.9470624200925315)]\n",
      "Moving DOWN\n",
      "Episode return: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "model = FrozenLakeModel(env.P)\n",
    "class FakeNetwork:\n",
    "    def __init__(self, nA):\n",
    "        self.act_probs = np.ones(nA) / sum(np.ones(nA))\n",
    "        self.value = np.random.rand()\n",
    "    \n",
    "    def __call__(self, obs):\n",
    "        return self.act_probs, self.value\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "ep_r = 0\n",
    "while not done:\n",
    "    action, root = MCTS_AlphaZero(obs, model, FakeNetwork(env.action_space.n), {\"iters\": 1000}).search()\n",
    "    print([(lake_actions[child.action], child.value) for child in root.children.values()])\n",
    "    print(f\"Moving {lake_actions[action]}\")\n",
    "    obs, r, done, _ = env.step(action)\n",
    "    ep_r += r\n",
    "\n",
    "print(f\"Episode return: {ep_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourRoomsModel:\n",
    "    def __init__(self, goal_pos=(16,16), seed=42):\n",
    "        self.goal_pos = goal_pos\n",
    "        self.seed = seed\n",
    "\n",
    "    def step(self, agent_state, action) -> tuple[_, float]:\n",
    "        agent_pos, agent_dir = agent_state\n",
    "        env = FourRoomsEnvPos(agent_pos=agent_pos, goal_pos=self.goal_pos)\n",
    "        env.reset(seed=self.seed)\n",
    "        env.agent_dir = agent_dir\n",
    "        return env.step(action)\n",
    "\n",
    "    def actions(self, obs) -> list[int]:\n",
    "        return list(range(env.action_space.n - 4))\n",
    "\n",
    "class FourRoomsEnvPos(FourRoomsEnv):\n",
    "    def __init__(self, agent_pos=None, goal_pos=None, **kwargs):\n",
    "        super().__init__(agent_pos=agent_pos, goal_pos=goal_pos, **kwargs)\n",
    "        self.max_steps = 1000\n",
    "    \n",
    "    def step(self, action):\n",
    "        _, r, terminated, truncated, _ = super().step(action)\n",
    "        return (self.agent_pos, self.agent_dir), int(self.agent_pos == self._goal_default_pos), terminated or truncated, _\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        obs, _ = super().reset(seed=seed)\n",
    "        return self.agent_pos, self.agent_dir\n",
    "\n",
    "\n",
    "action_map = {\n",
    "    0: \"left\",\n",
    "    1: \"right\",\n",
    "    2: \"forward\",\n",
    "    3: \"pickup\",\n",
    "    4: \"drop\",\n",
    "    5: \"toggle\",\n",
    "    6: \"done\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "class Fake:\n",
    "    def __init__(self):\n",
    "        self.string=\"hi\"\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.string\n",
    "\n",
    "a = Fake()\n",
    "print(a())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
