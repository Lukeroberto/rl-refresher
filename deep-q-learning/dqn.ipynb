{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from gym.spaces.utils import flatdim\n",
    "import torch as pt\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Comment out for debugging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(ep_number: int):\n",
    "    video = io.open(f\"./gym-results/rl-video-episode-{ep_number}.mp4\", 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return HTML(data='''\n",
    "        <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "    .format(encoded.decode('ascii')))\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='valid')\n",
    "    return y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "env = wrappers.RecordVideo(env, \"./gym-results\", new_step_api=True)\n",
    "env.reset(seed=42)\n",
    "\n",
    "# hyperparams\n",
    "num_steps = 500000\n",
    "random_rewards = [0]\n",
    "num_eps = 0\n",
    "for _ in trange(num_steps):\n",
    "   action = env.action_space.sample() \n",
    "   observation, reward, done, info, _ = env.step(action)\n",
    "   random_rewards.append(reward if reward > 0 else 0)\n",
    "\n",
    "   if done:\n",
    "      observation = env.reset()\n",
    "      num_eps += 1\n",
    "   \n",
    "env.close()\n",
    "\n",
    "plt.plot(smooth(random_rewards, 200))\n",
    "plt.title(f\"{sum(random_rewards)} wins out of {num_eps} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(random_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# Several useful wrapper environments\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "       \n",
    "def reset(self):\n",
    "    self.env.reset()\n",
    "    obs, _, done, _ = self.env.step(1)\n",
    "    if done:\n",
    "        self.env.reset()\n",
    "    obs, _, done, _ = self.env.step(2)\n",
    "    if done:\n",
    "        self.env.reset()\n",
    "    return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "           obs, reward, done, info, _ = self.env.step(action)\n",
    "           self._obs_buffer.append(obs)\n",
    "           total_reward += reward\n",
    "           if done:\n",
    "               break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info, _\n",
    "\n",
    "    def reset(self):\n",
    "       self._obs_buffer.clear()\n",
    "       obs = self.env.reset()\n",
    "       self._obs_buffer.append(obs)\n",
    "       return obs\n",
    "\n",
    "# TODO: Still produces broken ball and split user paddle\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, \n",
    "                            shape=(84, 84, 1), dtype=np.uint8)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "         \n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160,  3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\" \n",
    "\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110),            \n",
    "                        interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[16:100, :] # remove scoreboard + bottom\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,            \n",
    "                                shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]),\n",
    "                                dtype=np.float32)\n",
    "    def observation(self, observation):\n",
    "      return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "# Stacks several frames together\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = \\\n",
    "                 gym.spaces.Box(old_space.low.repeat(n_steps, \n",
    "                 axis=0),old_space.high.repeat(n_steps, axis=0),     \n",
    "                 dtype=dtype)\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low,\n",
    "        dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "     def observation(self, obs):\n",
    "         return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "def make_pong():\n",
    "   env = gym.make(\"ALE/Pong-v5\")\n",
    "   env = wrappers.RecordVideo(env, \"./gym-results\", new_step_api=True)\n",
    "   env = MaxAndSkipEnv(env, skip=2)\n",
    "   env = FireResetEnv(env)\n",
    "   env = ProcessFrame84(env)\n",
    "   env = ImageToPyTorch(env)\n",
    "   env = BufferWrapper(env, 4)\n",
    "   env = ScaledFloatFrame(env)\n",
    "\n",
    "   return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, obs_space, act_space):\n",
    "        super(QNet, self).__init__()\n",
    "        self.obs_shape = obs_space.low.shape\n",
    "        self.act_shape = flatdim(act_space)\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.obs_shape[0], 16, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2)\n",
    "        self.fc1 = nn.Linear(2592, 256) # mathemagic\n",
    "        self.out = nn.Linear(256, self.act_shape)\n",
    "\n",
    "        self.optimizer = pt.optim.RMSprop(self.parameters(), lr=1e-4)\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_vals, epsilon=0.1):\n",
    "    if pt.rand(1) < epsilon:\n",
    "        return pt.randint(6, (1,))\n",
    "    \n",
    "    return int(pt.argmax(q_vals).numpy())\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, size, obs_space, act_shape):\n",
    "        self.size = size\n",
    "        self.counter = 0\n",
    "\n",
    "        obs_shape = obs_space.shape\n",
    "\n",
    "        self.obs = np.zeros((size, *obs_shape))\n",
    "        self.actions = np.zeros((size))\n",
    "        self.rewards = np.zeros((size))\n",
    "        self.obs_n = np.zeros((size, *obs_shape))\n",
    "        self.done = np.zeros((size))\n",
    "    \n",
    "    def store_transition(self, obs, action, obs_n, r, done):\n",
    "        indx = self.counter % self.size\n",
    "\n",
    "        self.obs[indx] = obs\n",
    "        self.actions[indx] = action\n",
    "        self.rewards[indx] = r\n",
    "        self.obs_n[indx] = obs_n\n",
    "        self.done[indx] = done\n",
    "\n",
    "        self.counter += 1\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        max_mem = min(self.counter, self.size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        obs = pt.from_numpy(self.obs[batch]).float()\n",
    "        actions = pt.from_numpy(self.actions[batch]).long()\n",
    "        rewards = pt.from_numpy(self.rewards[batch]).float()\n",
    "        obs_n = pt.from_numpy(self.obs_n[batch]).float()\n",
    "        terminal = pt.from_numpy(self.done[batch]).bool()\n",
    "\n",
    "        return obs, actions, obs_n, rewards, terminal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test\n",
    "env = make_pong()\n",
    "obs = env.reset()\n",
    "memory = ReplayMemory(10, env.observation_space, env.action_space)\n",
    "\n",
    "dqn = QNet(env.observation_space, env.action_space)\n",
    "print(dqn)\n",
    "obs_n, r, done, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# q_vals = dqn(pt.tensor(obs))\n",
    "# action = epsilon_greedy(q_vals)\n",
    "action = 0\n",
    "\n",
    "print(f\"Storing transition: {obs.shape}, {action}, {obs_n.shape}, {r}, {done}\")\n",
    "memory.store_transition(obs, action, obs_n, r, done)\n",
    "memory.store_transition(obs, action, obs_n, r, done)\n",
    "memory.store_transition(obs, action, obs_n, r, done)\n",
    "memory.store_transition(obs, action, obs_n, r, done)\n",
    "memory.store_transition(obs, action, obs_n, r, done)\n",
    "memory.store_transition(obs, action, obs_n, r, done)\n",
    "states, actions, rewards, states_, dones = memory.sample_batch(6)\n",
    "print(f\"actions: {actions}\")\n",
    "q_vals = dqn(states)\n",
    "print(f\"All q-vals: {q_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode from DQN Paper\n",
    "env = make_pong()\n",
    "\n",
    "# Hyperparams\n",
    "epsilon = 0.99\n",
    "batch_size = 16\n",
    "update_freq = 2500\n",
    "gamma = 0.99\n",
    "\n",
    "# Initialize Replay Memory to capacity N\n",
    "replay_capacity = 1000\n",
    "memory = ReplayMemory(replay_capacity, env.observation_space, env.action_space)\n",
    "\n",
    "# Initialize action-value Function Q with random weights\n",
    "Q = QNet(env.observation_space, env.action_space)\n",
    "T = QNet(env.observation_space, env.action_space)\n",
    "\n",
    "num_episodes = 1000\n",
    "pbar = tqdm(range(num_episodes))\n",
    "dqn_rewards = [0]\n",
    "wins = [0]\n",
    "losses = [0]\n",
    "i = 0\n",
    "for ep in pbar:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "\n",
    "        # Select action random action with prob epsilon, otherwise argmax_a Q(obs, a)\n",
    "        q_vals = Q(pt.from_numpy(obs).unsqueeze(0).float())\n",
    "        # print(\"experienced q_val: \", q_vals)\n",
    "        action = epsilon_greedy(q_vals.detach(), epsilon=epsilon)\n",
    "\n",
    "        obs_n, r, done, _, _ = env.step(action)\n",
    "\n",
    "        ep_reward += r\n",
    "        wins.append(r if r > 0 else 0)\n",
    "        memory.store_transition(obs, action, obs_n, r, done)\n",
    "        obs = obs_n\n",
    "\n",
    "        cur_loss = 0\n",
    "        if (memory.counter > memory.size):\n",
    "            # Update network weights on batch\n",
    "            Q.optimizer.zero_grad()\n",
    "\n",
    "            states, actions, states_, rewards, dones = memory.sample_batch(batch_size)\n",
    "            indices = np.arange(batch_size)\n",
    "\n",
    "            q_vals = Q.forward(states)\n",
    "            # print(\"batch q_vals: \", q_vals)\n",
    "            q_pred = q_vals.gather(1, actions.unsqueeze(-1))\n",
    "            # print(\"selected action vals:\", q_pred)\n",
    "            q_next = T.forward(states_).detach().max(dim=1)[0]\n",
    "\n",
    "            q_target = rewards + gamma*dones*q_next\n",
    "            # print(\"target vals: \", q_target)\n",
    "\n",
    "            loss = Q.loss(q_target, q_pred)\n",
    "\n",
    "            pbar.set_description(f\"Epsilon: {epsilon:0.2f}, Win rate: {np.sum(wins)/ep:0.2f}, Rewards: {smooth(dqn_rewards, 50)[-1]}\")\n",
    "\n",
    "            loss.backward()\n",
    "            # for param in Q.parameters():\n",
    "            #     param.grad.data.clamp_(-1, 1)\n",
    "            Q.optimizer.step()\n",
    "            losses.append(loss.detach())\n",
    "        \n",
    "        if i % update_freq == 0:\n",
    "            T.load_state_dict(Q.state_dict())\n",
    "            T.eval()\n",
    "        \n",
    "    epsilon = max(0.05, epsilon - 1/500)\n",
    "    dqn_rewards.append(ep_reward)\n",
    "\n",
    "    if (ep % 20 == 0):\n",
    "        print(f\"batch Q vals: {q_vals}\")\n",
    "        plt.clf()\n",
    "        plt.plot(smooth(dqn_rewards, 10))\n",
    "        plt.savefig(f\"./gym-results/episode_{ep}.png\")\n",
    "\n",
    "        plt.clf()\n",
    "        plt.plot(smooth(losses, 1000))\n",
    "        plt.savefig(f\"./gym-results/losses_episode_{ep}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n if len(ret[n-1:]) > 0 else [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_pong()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    obs_format = pt.from_numpy(obs).unsqueeze(0).float()\n",
    "    q_vals = Q(obs_format)\n",
    "    action = pt.argmax(q_vals)\n",
    "\n",
    "    print(f\"action selected: {action}, vals: {q_vals}, obs: {np.sum(obs)}\")\n",
    "    obs, r, done, _, _ = env.step(action)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a926d3def179f663025bb8437b068ade0395ea661c618295799790ac4630b864"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
