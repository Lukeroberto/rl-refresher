{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from gym.spaces.utils import flatdim\n",
    "\n",
    "import torch as pt\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import collections\n",
    "\n",
    "# Comment out for debugging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear annealing schedule for random exploration\n",
    "def linear_schedule(start_e, end_e, total_steps, step):\n",
    "    slope = (end_e - start_e) / total_steps\n",
    "    return max(slope * step + start_e, end_e)\n",
    "\n",
    "# Replay memory to store transitions, in a cyclical buffer so olded transitions are removed first (Could be interesting to try a prioritized buffer instead)\n",
    "class ReplayMemory():\n",
    "    def __init__(self, size, env):\n",
    "        self.size = size\n",
    "        self.counter = 0\n",
    "\n",
    "        obs_shape = env.observation_space.shape\n",
    "\n",
    "        self.obs = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros((size, 1), env.action_space.dtype)\n",
    "        self.rewards = np.zeros((size, 1), dtype=np.float32)\n",
    "        self.obs_n = np.zeros((size, *obs_shape), dtype=np.float32)\n",
    "        self.done = np.zeros((size, 1), dtype=np.float32)\n",
    "    \n",
    "    def store_transition(self, obs, action, obs_n, r, done):\n",
    "        indx = self.counter % self.size\n",
    "\n",
    "        self.obs[indx] = np.array(obs).copy()\n",
    "        self.actions[indx] = np.array(action).copy()\n",
    "        self.rewards[indx] = np.array(r).copy()\n",
    "        self.obs_n[indx] = np.array(obs_n).copy()\n",
    "        self.done[indx] = np.array(done).copy()\n",
    "\n",
    "        self.counter += 1\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        obs = pt.as_tensor(self.obs[batch])\n",
    "        actions = pt.as_tensor(self.actions[batch])\n",
    "        rewards = pt.as_tensor(self.rewards[batch])\n",
    "        obs_n = pt.as_tensor(self.obs_n[batch])\n",
    "        terminal = pt.as_tensor(self.done[batch])\n",
    "\n",
    "        return obs, actions, obs_n, rewards, terminal\n",
    "\n",
    "# Simple Dense network for environments that have small, vector-based states\n",
    "class SimpleQNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, env.action_space.n)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Hparams that seem to work reasonably well, can definitely be tuned for quicker optimization\n",
    "DEFAULT_HPARAMS = {\n",
    "    \"training_steps\": 250_000,\n",
    "    \"batch_size\": 256,\n",
    "    \"update_freq\": 1000,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"buffer_size\": 50_000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_freq\": 4\n",
    "}\n",
    "\n",
    "def train_model(env, Q: SimpleQNet, hparams, path=\"./results\"):\n",
    "    # Pseudocode from DQN Paper\n",
    "    device = pt.device(\"cuda\" if pt.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter(path)\n",
    "\n",
    "    # Hyperparams\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    update_freq = hparams[\"update_freq\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "    buffer_size = hparams[\"buffer_size\"]\n",
    "    lr = hparams[\"learning_rate\"]\n",
    "    training_steps = hparams[\"training_steps\"]\n",
    "\n",
    "\n",
    "    # Intialize optimizer\n",
    "    optimizer = pt.optim.Adam(Q.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize Replay Memory to capacity N\n",
    "    memory = ReplayMemory(buffer_size, env)\n",
    "\n",
    "    # Initialize action-value Function Q with random weights\n",
    "    T = deepcopy(Q)\n",
    "\n",
    "    obs = env.reset()\n",
    "    ep_return = 0\n",
    "    ep_length = 0\n",
    "    for global_step in tqdm(range(training_steps)):\n",
    "\n",
    "        # Select action random action with prob epsilon, otherwise argmax_a Q(obs, a)\n",
    "        epsilon = linear_schedule(1.0, 0.05, 0.1 * training_steps ,global_step)\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_vals = Q(pt.Tensor(obs))\n",
    "            action = pt.argmax(q_vals).cpu().numpy()\n",
    "\n",
    "        obs_n, r, done, info = env.step(action)\n",
    "        memory.store_transition(obs, action, obs_n, r, done)\n",
    "\n",
    "        # Record statistics seems to be broken for some envs (TODO: find out why borked)\n",
    "        ep_return += r\n",
    "        ep_length += 1\n",
    "        if \"episode\" in info.keys():\n",
    "            writer.add_scalar(\"charts/episodic_return\", ep_return, global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", ep_length, global_step)\n",
    "            writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
    "\n",
    "        obs = obs_n\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            ep_return = 0\n",
    "            ep_length = 0\n",
    "\n",
    "        # Fill up buffer before we start training\n",
    "        if (memory.counter > memory.size and global_step % hparams[\"train_freq\"] == 0):\n",
    "\n",
    "            # Update network weights on batch\n",
    "            states, actions, states_, rewards, dones = memory.sample_batch(batch_size)\n",
    "\n",
    "            # Target values\n",
    "            with pt.no_grad():\n",
    "                q_next, _ = T.forward(states_).max(dim=1)\n",
    "                q_next = q_next.reshape(-1, 1)\n",
    "                q_target = rewards + gamma*(1-dones)*q_next\n",
    "\n",
    "            # Predicted values\n",
    "            q_pred = Q.forward(states)\n",
    "            q_pred = pt.gather(q_pred, dim=1, index=actions)\n",
    "\n",
    "            loss = F.smooth_l1_loss(q_pred, q_target)\n",
    "        \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Stable baselines:  https://github.com/DLR-RM/stable-baselines3/blob/4fb8aec215fd2dd5d668aae8285937c268baca97/stable_baselines3/dqn/dqn.py#L216\n",
    "            # Didn't prove too useful for envs I tried\n",
    "            # nn.utils.clip_grad_norm_(Q.parameters(), 10)             \n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 200 == 0:\n",
    "                grads = [pt.flatten(params.grad.detach()) for params in Q.parameters()]\n",
    "                grads = pt.cat(grads).view(-1,1)\n",
    "                writer.add_histogram(\"charts/action_dist\", actions, global_step)\n",
    "                writer.add_scalar(\"losses/grads\", np.linalg.norm(grads.numpy()), global_step)\n",
    "                writer.add_scalar(\"losses/td_loss\", loss.detach().numpy(), global_step)\n",
    "                writer.add_scalar(\"losses/value_fn\", q_pred.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/q_vals [max]\", q_pred.max().item(), global_step)\n",
    "\n",
    "            if global_step % update_freq == 0:\n",
    "                T.load_state_dict(Q.state_dict())\n",
    "    \n",
    "    env.close()\n",
    "    writer.close()\n",
    "    pt.save(Q, path + f\"/q_{training_steps}.pt\")\n",
    "    return Q\n",
    "    \n",
    "def generate_video(net, env, path=\"test\"):\n",
    "    env = gym.wrappers.RecordVideo(env, path)\n",
    "    obs= env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        q_vals = net(pt.Tensor(obs).unsqueeze(0))\n",
    "        action = pt.argmax(q_vals).cpu().numpy()\n",
    "        obs, _, done, info = env.step(action)\n",
    "    \n",
    "    env.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on acrobot environment: https://gymnasium.farama.org/environments/classic_control/acrobot/\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "hparams = DEFAULT_HPARAMS\n",
    "hparams[\"training_steps\"] = 60_000\n",
    "q = train_model(env, SimpleQNet(env), hparams, path=\"./acrobot\")\n",
    "\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "generate_video(q, env, \"./acrobot/eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on cartpole environment: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "q = train_model(env, SimpleQNet(env), DEFAULT_HPARAMS, path=\"./cartpole\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"./cartpole\")\n",
    "generate_video(q, env, \"./cartpol/eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baselines wrappers had some incompatibility issues so pulled these from: https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\n",
    "###############################\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "       super(FireResetEnv, self).__init__(env)\n",
    "       assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "       assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "       return self.env.step(action)\n",
    "       \n",
    "    def reset(self):\n",
    "       self.env.reset()\n",
    "       obs, _, done, _ = self.env.step(1)\n",
    "       if done:\n",
    "          self.env.reset()\n",
    "       obs, _, done, _ = self.env.step(2)\n",
    "       if done:\n",
    "          self.env.reset()\n",
    "       return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "           obs, reward, done, info = self.env.step(action)\n",
    "           self._obs_buffer.append(obs)\n",
    "           total_reward += reward\n",
    "           if done:\n",
    "               break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "        \n",
    "    def reset(self):\n",
    "       self._obs_buffer.clear()\n",
    "       obs = self.env.reset()\n",
    "       self._obs_buffer.append(obs)\n",
    "       return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "         super(ProcessFrame84, self).__init__(env)\n",
    "         self.observation_space = gym.spaces.Box(low=0, high=255, \n",
    "                               shape=(84, 84, 1), dtype=np.uint8)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "         return ProcessFrame84.process(obs)\n",
    "         \n",
    "         \n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "             img = np.reshape(frame, [210, 160,  3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "             img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "             assert False, \"Unknown resolution.\"       \n",
    "\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110),            \n",
    "                        interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, \n",
    "                 axis=0),old_space.high.repeat(n_steps, axis=0),     \n",
    "                 dtype=dtype)\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low,\n",
    "        dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,            \n",
    "                                shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]),\n",
    "                                dtype=np.float32)\n",
    "    def observation(self, observation):\n",
    "      return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "     def observation(self, obs):\n",
    "         return np.array(obs).astype(np.float32) / 255.0\n",
    "###############################\n",
    "\n",
    "def make_env(env_id):\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env) \n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)\n",
    "\n",
    "# Convolutional Network to handle pixel-observations, taken from original DQN paper roughly\n",
    "class ConvQNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ConvQNet, self).__init__()\n",
    "        self.obs_shape = env.observation_space.low.shape\n",
    "        self.act_shape = flatdim(env.action_space)\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.obs_shape[0], 16, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2)\n",
    "        self.fc1 = nn.Linear(2592, 256) # mathemagic\n",
    "        self.out = nn.Linear(256, self.act_shape)\n",
    "\n",
    "        self.optimizer = pt.optim.RMSprop(self.parameters(), lr=1e-4)\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Atari envs are not up to the new gym API\n",
    "def train_atari_model(env, Q: ConvQNet, hparams):\n",
    "    # Pseudocode from DQN Paper\n",
    "    device = pt.device(\"cuda\" if pt.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter(\"pong-contd\")\n",
    "\n",
    "    # Hyperparams\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    update_freq = hparams[\"update_freq\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "    buffer_size = hparams[\"buffer_size\"]\n",
    "    lr = hparams[\"learning_rate\"]\n",
    "    training_steps = hparams[\"training_steps\"]\n",
    "\n",
    "\n",
    "    # Intialize optimizer\n",
    "    optimizer = pt.optim.Adam(Q.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize Replay Memory to capacity N\n",
    "    memory = ReplayMemory(buffer_size, env)\n",
    "\n",
    "    # Initialize action-value Function Q with random weights\n",
    "    T = deepcopy(Q)\n",
    "\n",
    "    obs = env.reset()\n",
    "    ep_return = 0\n",
    "    ep_length = 0\n",
    "    for global_step in tqdm(range(training_steps)):\n",
    "\n",
    "        # Select action random action with prob epsilon, otherwise argmax_a Q(obs, a)\n",
    "        epsilon = linear_schedule(1.0, 0.05, 0.1 * training_steps ,global_step)\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_vals = Q(pt.Tensor(obs).unsqueeze(0))\n",
    "            action = pt.argmax(q_vals).cpu().numpy()\n",
    "\n",
    "        obs_n, r, done, info = env.step(action)\n",
    "\n",
    "        # Record statistics seems to be broken for some envs\n",
    "        ep_return += r\n",
    "        ep_length += 1\n",
    "\n",
    "        if \"episode\" in info.keys():\n",
    "            writer.add_scalar(\"charts/episodic_return\", ep_return, global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", ep_length, global_step)\n",
    "            writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
    "\n",
    "        memory.store_transition(obs, action, obs_n, r, done)\n",
    "\n",
    "        # handle terminal observations\n",
    "        obs = obs_n\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            ep_return = 0\n",
    "            ep_length = 0\n",
    "\n",
    "        if (memory.counter > memory.size and global_step % hparams[\"train_freq\"] == 0):\n",
    "\n",
    "            # Update network weights on batch\n",
    "            states, actions, states_, rewards, dones = memory.sample_batch(batch_size)\n",
    "\n",
    "            # Target values\n",
    "            with pt.no_grad():\n",
    "                q_next, _ = T.forward(states_).max(dim=1)\n",
    "                q_next = q_next.reshape(-1, 1)\n",
    "                q_target = rewards + gamma*(1-dones)*q_next\n",
    "\n",
    "            # Predicted values\n",
    "            q_pred = Q.forward(states)\n",
    "            q_pred = pt.gather(q_pred, dim=1, index=actions)\n",
    "\n",
    "            loss = F.smooth_l1_loss(q_pred, q_target)\n",
    "        \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Stable baselines:  https://github.com/DLR-RM/stable-baselines3/blob/4fb8aec215fd2dd5d668aae8285937c268baca97/stable_baselines3/dqn/dqn.py#L216\n",
    "            # nn.utils.clip_grad_norm_(Q.parameters(), 10)             \n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 200 == 0:\n",
    "                grads = [pt.flatten(params.grad.detach()) for params in Q.parameters()]\n",
    "                grads = pt.cat(grads).view(-1,1)\n",
    "                writer.add_histogram(\"charts/action_dist\", actions, global_step)\n",
    "                writer.add_scalar(\"losses/grads\", np.linalg.norm(grads.numpy()), global_step)\n",
    "                writer.add_scalar(\"losses/td_loss\", loss.detach().numpy(), global_step)\n",
    "                writer.add_scalar(\"losses/value_fn\", q_pred.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/q_vals [max]\", q_pred.max().item(), global_step)\n",
    "\n",
    "            if global_step % update_freq == 0:\n",
    "                T.load_state_dict(Q.state_dict())\n",
    "    \n",
    "    env.close()\n",
    "    writer.close()\n",
    "    return Q\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "def generate_video(net, env, path=\"test\"):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    frames = [env.render(mode=\"rgb_array\")]\n",
    "    while not done:\n",
    "        q_vals = net(pt.Tensor(obs).unsqueeze(0))\n",
    "        action = pt.argmax(q_vals).cpu().numpy()\n",
    "\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "    \n",
    "    clip = ImageSequenceClip(frames, fps=20)\n",
    "    clip.write_gif(path + '.gif', fps=20)\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"ALE/Pong-v5\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "q = ConvQNet(env)\n",
    "\n",
    "params = DEFAULT_HPARAMS\n",
    "params[\"buffer_size\"] = 50_000\n",
    "params[\"training_steps\"] = 1_000_000\n",
    "\n",
    "q = train_atari_model(env, q, params)\n",
    "\n",
    "generate_video(q, env, \"./pong/eval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
