{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from gym.spaces.utils import flatdim\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import collections\n",
    "\n",
    "# Comment out for debugging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear annealing schedule for random exploration\n",
    "def linear_schedule(start_e, end_e, total_steps, step):\n",
    "    slope = (end_e - start_e) / total_steps\n",
    "    return max(slope * step + start_e, end_e)\n",
    "\n",
    "# Simple Dense network for environments that have small, vector-based states\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.Categorical(logits=logits)\n",
    "        if action == None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "\n",
    "# Hparams that seem to work reasonably well, can definitely be tuned for quicker optimization\n",
    "DEFAULT_HPARAMS = {\n",
    "    \"training_steps\": 500_000,\n",
    "    \"rollout_length\": 128,\n",
    "    \"mini_batch_size\": 4,\n",
    "    \"gamma\": 0.9,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_coef\": 0.2,\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"epochs\": 4\n",
    "}\n",
    "\n",
    "def train_model(env, agent: ActorCriticNet, hparams, path=\"./results\"):\n",
    "    # Pseudocode from DQN Paper\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    writer = SummaryWriter(path)\n",
    "\n",
    "    # Hyperparams\n",
    "    training_steps = hparams[\"training_steps\"]\n",
    "    rollout_length = hparams[\"rollout_length\"]\n",
    "    mini_batch_size = hparams[\"mini_batch_size\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "    gae_lambda = hparams[\"gae_lambda\"]\n",
    "    clip_coef = hparams[\"clip_coef\"]\n",
    "    lr = hparams[\"learning_rate\"]\n",
    "    epochs = hparams[\"epochs\"]\n",
    "\n",
    "    batch_size = rollout_length\n",
    "\n",
    "    # Intialize optimizer\n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize Storage vectors\n",
    "    obs = torch.zeros((rollout_length,  *env.observation_space.shape)).to(device)\n",
    "    actions = torch.zeros((rollout_length, *env.observation_space.shape)).to(device)\n",
    "    logprobs = torch.zeros((rollout_length, 1)).to(device)\n",
    "    rewards = torch.zeros((rollout_length, 1)).to(device)\n",
    "    dones = torch.zeros((rollout_length, 1)).to(device)\n",
    "    values = torch.zeros((rollout_length, 1)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    obs_n = torch.Tensor(env.reset()).to(device)\n",
    "    done = torch.Tensor([0]).to(device)\n",
    "    ep_return = 0\n",
    "    ep_length = 0\n",
    "    for step in tqdm(range(training_steps)):\n",
    "        epsilon = linear_schedule(1.0, 0.05, 0.1 * training_steps ,step)\n",
    "\n",
    "        # Env rollouts\n",
    "        #########################################################\n",
    "        for env_step in range(rollout_length):\n",
    "            obs[step] = obs_n\n",
    "            dones[step] = done\n",
    "\n",
    "            # Select action \n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.action_and_value(obs_n)\n",
    "\n",
    "                # Log this here so we dont track grad\n",
    "                values[env_step] = value.flatten()\n",
    "            \n",
    "\n",
    "            obs_n, r, done, info = env.step(action.cpu().numpy())\n",
    "\n",
    "            obs_n = torch.Tensor(obs_n).to(device)\n",
    "            done = torch.Tensor([done]).to(device)\n",
    "\n",
    "            actions[env_step] = action\n",
    "            logprobs[env_step] = logprob\n",
    "            rewards[env_step] = torch.tensor(r).to(device).view(-1)\n",
    "\n",
    "            # Record statistics seems to be broken for some envs (TODO: find out why borked)\n",
    "            ep_return += r\n",
    "            ep_length += 1\n",
    "            if \"episode\" in info.keys():\n",
    "                writer.add_scalar(\"charts/episodic_return\", ep_return, step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", ep_length, step)\n",
    "\n",
    "            if done:\n",
    "                obs_n = env.reset()\n",
    "                ep_return = 0\n",
    "                ep_length = 0\n",
    "\n",
    "        #######################################################\n",
    "\n",
    "        # Compute advantages/returns\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.value(obs[-1]).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "\n",
    "            for t in reversed(range(rollout_length)):\n",
    "                if t == rollout_length - 1:\n",
    "                    nextnonterminal = 1.0 - dones[-1]\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "\n",
    "                bellman_error = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                lastgaelam = bellman_error + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                advantages[t] = lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "\n",
    "        # Flatten rollout batch\n",
    "        batch_obs = obs.reshape((-1,) + env.observation_space.shape)\n",
    "        batch_logprobs = logprobs.reshape(-1)\n",
    "        batch_actions = actions.reshape((-1,) + env.action_space.shape)\n",
    "        batch_advantages = advantages.reshape(-1)\n",
    "        batch_returns = returns.reshape(-1)\n",
    "        batch_values = values.reshape(-1)\n",
    "\n",
    "        # Optimize Agent\n",
    "        #######################################################\n",
    "        batch_inds = np.arange(batch_size)\n",
    "        clipped = []\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(batch_inds)\n",
    "            for ind_start in range(0, batch_size, mini_batch_size):\n",
    "                ind_end = ind_start + mini_batch_size\n",
    "                batch = batch_inds[ind_start:ind_end]\n",
    "\n",
    "                # Compute approx KL divergence of batch, use for clip factor\n",
    "                _, new_logprobs, entropy, new_values = agent.actor_and_critic(batch_obs[batch], batch_actions.long()[batch])\n",
    "                logratio = new_logprobs - batch_logprobs[batch] # Difference in policy probs\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "\n",
    "                # https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "\n",
    "                # Policy loss\n",
    "                policy_loss_a = - batch_advantages[batch] * ratio\n",
    "                policy_loss_b = - batch_advantages[batch] * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                policy_loss = torch.max(policy_loss_a, policy_loss_b).mean()\n",
    "\n",
    "                # Value Loss\n",
    "                new_values = new_values.view(-1)\n",
    "                value_loss = 0.5 * F.mse_loss(new_values, batch_returns[batch])\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                # Weights from link above\n",
    "                loss = policy_loss - 0.01 * entropy_loss + 0.5 * value_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.item(), step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), step)\n",
    "        # writer.add_scalar(\"losses/explained_variance\", explained_var, step)\n",
    "\n",
    "        #######################################################\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n",
    "    torch.save(agent, path + f\"/q_{training_steps}.pt\")\n",
    "    return agent\n",
    "    \n",
    "def generate_video(net, env, path=\"test\"):\n",
    "    env = gym.wrappers.RecordVideo(env, path)\n",
    "    obs= env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        q_vals = net(pt.Tensor(obs).unsqueeze(0))\n",
    "        action = pt.argmax(q_vals).cpu().numpy()\n",
    "        obs, _, done, info = env.step(action)\n",
    "    \n",
    "    env.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ActorCriticNet' object has no attribute 'actor_and_critic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mwrappers\u001b[39m.\u001b[39mRecordEpisodeStatistics(env)\n\u001b[1;32m      5\u001b[0m hparams \u001b[39m=\u001b[39m DEFAULT_HPARAMS\n\u001b[0;32m----> 6\u001b[0m agent \u001b[39m=\u001b[39m train_model(env, ActorCriticNet(env), hparams, path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./acrobot\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# env = gym.make(\"Acrobot-v1\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# generate_video(q, env, \"./acrobot/eval\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m (hparams[\u001b[39m\"\u001b[39m\u001b[39mtraining_steps\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn [33], line 164\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(env, agent, hparams, path)\u001b[0m\n\u001b[1;32m    161\u001b[0m batch \u001b[39m=\u001b[39m batch_inds[ind_start:ind_end]\n\u001b[1;32m    163\u001b[0m \u001b[39m# Compute approx KL divergence of batch, use for clip factor\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m _, new_logprobs, entropy, new_values \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mactor_and_critic(batch_obs[batch], batch_actions\u001b[39m.\u001b[39mlong()[batch])\n\u001b[1;32m    165\u001b[0m logratio \u001b[39m=\u001b[39m new_logprobs \u001b[39m-\u001b[39m batch_logprobs[batch] \u001b[39m# Difference in policy probs\u001b[39;00m\n\u001b[1;32m    166\u001b[0m ratio \u001b[39m=\u001b[39m logratio\u001b[39m.\u001b[39mexp()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ActorCriticNet' object has no attribute 'actor_and_critic'"
     ]
    }
   ],
   "source": [
    "# Test on acrobot environment: https://gymnasium.farama.org/environments/classic_control/acrobot/\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "hparams = DEFAULT_HPARAMS\n",
    "agent = train_model(env, ActorCriticNet(env), hparams, path=\"./acrobot\")\n",
    "\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "# generate_video(q, env, \"./acrobot/eval\")\n",
    "(hparams[\"training_steps\"], 1) + env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on cartpole environment: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "q = train_model(env, SimpleQNet(env), DEFAULT_HPARAMS, path=\"./cartpole\")\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"./cartpole\")\n",
    "generate_video(q, env, \"./cartpol/eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baselines wrappers had some incompatibility issues so pulled these from: https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af\n",
    "###############################\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "       super(FireResetEnv, self).__init__(env)\n",
    "       assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "       assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "       return self.env.step(action)\n",
    "       \n",
    "    def reset(self):\n",
    "       self.env.reset()\n",
    "       obs, _, done, _ = self.env.step(1)\n",
    "       if done:\n",
    "          self.env.reset()\n",
    "       obs, _, done, _ = self.env.step(2)\n",
    "       if done:\n",
    "          self.env.reset()\n",
    "       return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "           obs, reward, done, info = self.env.step(action)\n",
    "           self._obs_buffer.append(obs)\n",
    "           total_reward += reward\n",
    "           if done:\n",
    "               break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "        \n",
    "    def reset(self):\n",
    "       self._obs_buffer.clear()\n",
    "       obs = self.env.reset()\n",
    "       self._obs_buffer.append(obs)\n",
    "       return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "         super(ProcessFrame84, self).__init__(env)\n",
    "         self.observation_space = gym.spaces.Box(low=0, high=255, \n",
    "                               shape=(84, 84, 1), dtype=np.uint8)\n",
    "    \n",
    "    def observation(self, obs):\n",
    "         return ProcessFrame84.process(obs)\n",
    "         \n",
    "         \n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "             img = np.reshape(frame, [210, 160,  3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "             img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "             assert False, \"Unknown resolution.\"       \n",
    "\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110),            \n",
    "                        interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, \n",
    "                 axis=0),old_space.high.repeat(n_steps, axis=0),     \n",
    "                 dtype=dtype)\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low,\n",
    "        dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,            \n",
    "                                shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]),\n",
    "                                dtype=np.float32)\n",
    "    def observation(self, observation):\n",
    "      return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "     def observation(self, obs):\n",
    "         return np.array(obs).astype(np.float32) / 255.0\n",
    "###############################\n",
    "\n",
    "def make_env(env_id):\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env) \n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)\n",
    "\n",
    "# Convolutional Network to handle pixel-observations, taken from original DQN paper roughly\n",
    "class ConvQNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ConvQNet, self).__init__()\n",
    "        self.obs_shape = env.observation_space.low.shape\n",
    "        self.act_shape = flatdim(env.action_space)\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.obs_shape[0], 16, 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2)\n",
    "        self.fc1 = nn.Linear(2592, 256) # mathemagic\n",
    "        self.out = nn.Linear(256, self.act_shape)\n",
    "\n",
    "        self.optimizer = pt.optim.RMSprop(self.parameters(), lr=1e-4)\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Atari envs are not up to the new gym API\n",
    "def train_atari_model(env, Q: ConvQNet, hparams):\n",
    "    # Pseudocode from DQN Paper\n",
    "    device = pt.device(\"cuda\" if pt.cuda.is_available() else \"cpu\")\n",
    "    writer = SummaryWriter(\"pong-contd\")\n",
    "\n",
    "    # Hyperparams\n",
    "    batch_size = hparams[\"batch_size\"]\n",
    "    update_freq = hparams[\"update_freq\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "    buffer_size = hparams[\"buffer_size\"]\n",
    "    lr = hparams[\"learning_rate\"]\n",
    "    training_steps = hparams[\"training_steps\"]\n",
    "\n",
    "\n",
    "    # Intialize optimizer\n",
    "    optimizer = pt.optim.Adam(Q.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize Replay Memory to capacity N\n",
    "    memory = ReplayMemory(buffer_size, env)\n",
    "\n",
    "    # Initialize action-value Function Q with random weights\n",
    "    T = deepcopy(Q)\n",
    "\n",
    "    obs = env.reset()\n",
    "    ep_return = 0\n",
    "    ep_length = 0\n",
    "    for global_step in tqdm(range(training_steps)):\n",
    "\n",
    "        # Select action random action with prob epsilon, otherwise argmax_a Q(obs, a)\n",
    "        epsilon = linear_schedule(1.0, 0.05, 0.1 * training_steps ,global_step)\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_vals = Q(pt.Tensor(obs).unsqueeze(0))\n",
    "            action = pt.argmax(q_vals).cpu().numpy()\n",
    "\n",
    "        obs_n, r, done, info = env.step(action)\n",
    "\n",
    "        # Record statistics seems to be broken for some envs\n",
    "        ep_return += r\n",
    "        ep_length += 1\n",
    "\n",
    "        if \"episode\" in info.keys():\n",
    "            writer.add_scalar(\"charts/episodic_return\", ep_return, global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", ep_length, global_step)\n",
    "            writer.add_scalar(\"charts/epsilon\", epsilon, global_step)\n",
    "\n",
    "        memory.store_transition(obs, action, obs_n, r, done)\n",
    "\n",
    "        # handle terminal observations\n",
    "        obs = obs_n\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            ep_return = 0\n",
    "            ep_length = 0\n",
    "\n",
    "        if (memory.counter > memory.size and global_step % hparams[\"train_freq\"] == 0):\n",
    "\n",
    "            # Update network weights on batch\n",
    "            states, actions, states_, rewards, dones = memory.sample_batch(batch_size)\n",
    "\n",
    "            # Target values\n",
    "            with pt.no_grad():\n",
    "                q_next, _ = T.forward(states_).max(dim=1)\n",
    "                q_next = q_next.reshape(-1, 1)\n",
    "                q_target = rewards + gamma*(1-dones)*q_next\n",
    "\n",
    "            # Predicted values\n",
    "            q_pred = Q.forward(states)\n",
    "            q_pred = pt.gather(q_pred, dim=1, index=actions)\n",
    "\n",
    "            loss = F.smooth_l1_loss(q_pred, q_target)\n",
    "        \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Stable baselines:  https://github.com/DLR-RM/stable-baselines3/blob/4fb8aec215fd2dd5d668aae8285937c268baca97/stable_baselines3/dqn/dqn.py#L216\n",
    "            # nn.utils.clip_grad_norm_(Q.parameters(), 10)             \n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 200 == 0:\n",
    "                grads = [pt.flatten(params.grad.detach()) for params in Q.parameters()]\n",
    "                grads = pt.cat(grads).view(-1,1)\n",
    "                writer.add_histogram(\"charts/action_dist\", actions, global_step)\n",
    "                writer.add_scalar(\"losses/grads\", np.linalg.norm(grads.numpy()), global_step)\n",
    "                writer.add_scalar(\"losses/td_loss\", loss.detach().numpy(), global_step)\n",
    "                writer.add_scalar(\"losses/value_fn\", q_pred.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/q_vals [max]\", q_pred.max().item(), global_step)\n",
    "\n",
    "            if global_step % update_freq == 0:\n",
    "                T.load_state_dict(Q.state_dict())\n",
    "    \n",
    "    env.close()\n",
    "    writer.close()\n",
    "    return Q\n",
    "\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "def generate_video(net, env, path=\"test\"):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    frames = [env.render(mode=\"rgb_array\")]\n",
    "    while not done:\n",
    "        q_vals = net(pt.Tensor(obs).unsqueeze(0))\n",
    "        action = pt.argmax(q_vals).cpu().numpy()\n",
    "\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "    \n",
    "    clip = ImageSequenceClip(frames, fps=20)\n",
    "    clip.write_gif(path + '.gif', fps=20)\n",
    "    env.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"ALE/Pong-v5\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "q = ConvQNet(env)\n",
    "\n",
    "params = DEFAULT_HPARAMS\n",
    "params[\"buffer_size\"] = 50_000\n",
    "params[\"training_steps\"] = 1_000_000\n",
    "\n",
    "q = train_atari_model(env, q, params)\n",
    "\n",
    "generate_video(q, env, \"./pong/eval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
