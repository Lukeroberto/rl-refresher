{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from gym.spaces.utils import flatdim\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import collections\n",
    "\n",
    "# Comment out for debugging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear annealing schedule for random exploration\n",
    "def linear_schedule(start_e, end_e, total_steps, step):\n",
    "    slope = (end_e - start_e) / total_steps\n",
    "    return max(slope * step + start_e, end_e)\n",
    "\n",
    "# Simple Dense network for environments that have small, vector-based states\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.array(env.observation_space.shape).prod(), 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = torch.distributions.Categorical(logits=logits)\n",
    "        if action == None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "\n",
    "# Hparams that seem to work reasonably well, can definitely be tuned for quicker optimization\n",
    "DEFAULT_HPARAMS = {\n",
    "    \"training_steps\": 500_000,\n",
    "    \"rollout_length\": 128,\n",
    "    \"mini_batch_size\": 4,\n",
    "    \"gamma\": 0.9,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_coef\": 0.2,\n",
    "    \"learning_rate\": 2.5e-4,\n",
    "    \"epochs\": 4\n",
    "}\n",
    "\n",
    "def train_model(env, agent: ActorCriticNet, hparams, path=\"./results\"):\n",
    "    # Pseudocode from DQN Paper\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    writer = SummaryWriter(path)\n",
    "\n",
    "    # Hyperparams\n",
    "    training_steps = hparams[\"training_steps\"]\n",
    "    rollout_length = hparams[\"rollout_length\"]\n",
    "    mini_batch_size = hparams[\"mini_batch_size\"]\n",
    "    gamma = hparams[\"gamma\"]\n",
    "    gae_lambda = hparams[\"gae_lambda\"]\n",
    "    clip_coef = hparams[\"clip_coef\"]\n",
    "    lr = hparams[\"learning_rate\"]\n",
    "    epochs = hparams[\"epochs\"]\n",
    "\n",
    "    batch_size = rollout_length\n",
    "    training_steps = int(training_steps / batch_size)\n",
    "\n",
    "    # Intialize optimizer\n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize Storage vectors\n",
    "    obs = torch.zeros((rollout_length,  *env.observation_space.shape)).to(device)\n",
    "    actions = torch.zeros((rollout_length, 1)).to(device)\n",
    "    logprobs = torch.zeros((rollout_length, 1)).to(device)\n",
    "    rewards = torch.zeros((rollout_length, 1)).to(device)\n",
    "    dones = torch.zeros((rollout_length, 1)).to(device)\n",
    "    values = torch.zeros((rollout_length, 1)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    obs_n = torch.Tensor(env.reset()).to(device)\n",
    "    done = torch.zeros(1).to(device)\n",
    "    ep_return = 0\n",
    "    ep_length = 0\n",
    "    global_step = 0\n",
    "    for step in tqdm(range(training_steps)):\n",
    "        epsilon = linear_schedule(1.0, 0.05, 0.1 * training_steps ,step)\n",
    "\n",
    "        # Env rollouts\n",
    "        #########################################################\n",
    "        for env_step in range(rollout_length):\n",
    "            global_step += 1\n",
    "            obs[env_step] = obs_n\n",
    "            dones[env_step] = done\n",
    "\n",
    "            # Select action \n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.action_and_value(obs_n)\n",
    "\n",
    "                # Log this here so we dont track grad\n",
    "                values[env_step] = value.flatten()\n",
    "            \n",
    "            # print(\"Action taken: \", action)\n",
    "            actions[env_step] = action\n",
    "            logprobs[env_step] = logprob\n",
    "\n",
    "            obs_n, r, done, info = env.step(action.cpu().numpy())\n",
    "            obs_n = torch.Tensor(obs_n).to(device)\n",
    "            done = torch.Tensor([done]).to(device)\n",
    "\n",
    "            rewards[env_step] = torch.tensor(r).to(device).view(-1)\n",
    "\n",
    "            # Record statistics seems to be broken for some envs (TODO: find out why borked)\n",
    "            ep_return += r\n",
    "            ep_length += 1\n",
    "            if \"episode\" in info.keys():\n",
    "                writer.add_scalar(\"charts/episodic_return\", ep_return, global_step)\n",
    "                writer.add_scalar(\"charts/episodic_length\", ep_length, global_step)\n",
    "\n",
    "            if done:\n",
    "                obs_n = torch.Tensor(env.reset()).to(device)\n",
    "                ep_return = 0\n",
    "                ep_length = 0\n",
    "\n",
    "        #######################################################\n",
    "\n",
    "        # Compute advantages/returns\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.value(obs[-1]).reshape(1, -1)\n",
    "\n",
    "            # Essentially Q - V, + good action/ - bad action\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "\n",
    "            for t in reversed(range(rollout_length)):\n",
    "                if t == rollout_length - 1:\n",
    "                    nextnonterminal = 1.0 - done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "\n",
    "                bellman_error = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                lastgaelam = bellman_error + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                advantages[t] = lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "\n",
    "        # Flatten rollout batch\n",
    "        batch_obs = obs.reshape((-1,) + env.observation_space.shape)\n",
    "        batch_logprobs = logprobs.reshape(-1)\n",
    "        batch_actions = actions.reshape((-1,) + env.action_space.shape)\n",
    "        batch_advantages = advantages.reshape(-1)\n",
    "        batch_returns = returns.reshape(-1)\n",
    "        batch_values = values.reshape(-1)\n",
    "\n",
    "        # print(\"Batch: \")\n",
    "        # print(batch_obs.shape)\n",
    "        # print(batch_logprobs.shape)\n",
    "        # print(batch_actions.shape)\n",
    "        # print(batch_advantages.shape)\n",
    "        # print(batch_returns.shape)\n",
    "        # print(batch_values.shape)\n",
    "\n",
    "        # Optimize Agent\n",
    "        #######################################################\n",
    "        batch_inds = np.arange(batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(batch_inds)\n",
    "            for ind_start in range(0, batch_size, mini_batch_size):\n",
    "                ind_end = ind_start + mini_batch_size\n",
    "                batch = batch_inds[ind_start:ind_end]\n",
    "\n",
    "                # Compute approx KL divergence of batch, use for clip factor\n",
    "                _, new_logprobs, entropy, new_values = agent.action_and_value(batch_obs[batch], batch_actions.long()[batch])\n",
    "                logratio = new_logprobs - batch_logprobs[batch] # Difference in policy probs\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "\n",
    "                # https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "\n",
    "                # Policy loss\n",
    "                policy_loss_a = - batch_advantages[batch] * ratio\n",
    "                policy_loss_b = - batch_advantages[batch] * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                policy_loss = torch.max(policy_loss_a, policy_loss_b).mean()\n",
    "\n",
    "                # Value Loss\n",
    "                new_values = new_values.view(-1)\n",
    "                value_loss = 0.5 * F.mse_loss(new_values, batch_returns[batch])\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                # Weights from link above\n",
    "                loss = policy_loss - 0.01 * entropy_loss + 0.5 * value_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "                \n",
    "        y_pred, y_true = batch_values.cpu().numpy(), batch_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "\n",
    "        #######################################################\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n",
    "    torch.save(agent, path + f\"/q_{training_steps}.pt\")\n",
    "    return agent\n",
    "    \n",
    "def generate_video(net, env, path=\"test\"):\n",
    "    env = gym.wrappers.RecordVideo(env, path)\n",
    "    obs= env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        q_vals = net(torch.Tensor(obs).unsqueeze(0))\n",
    "        action =torch.argmax(q_vals).cpu().numpy()\n",
    "        obs, _, done, info = env.step(action)\n",
    "    \n",
    "    env.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1148/3906 [10:20<24:51,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mwrappers\u001b[39m.\u001b[39mRecordEpisodeStatistics(env)\n\u001b[1;32m      5\u001b[0m hparams \u001b[39m=\u001b[39m DEFAULT_HPARAMS\n\u001b[0;32m----> 6\u001b[0m agent \u001b[39m=\u001b[39m train_model(env, ActorCriticNet(env), hparams, path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./acrobot\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# env = gym.make(\"Acrobot-v1\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# generate_video(q, env, \"./acrobot/eval\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m (hparams[\u001b[39m\"\u001b[39m\u001b[39mtraining_steps\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn [20], line 87\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(env, agent, hparams, path)\u001b[0m\n\u001b[1;32m     85\u001b[0m ep_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     86\u001b[0m global_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(training_steps)):\n\u001b[1;32m     88\u001b[0m     epsilon \u001b[39m=\u001b[39m linear_schedule(\u001b[39m1.0\u001b[39m, \u001b[39m0.05\u001b[39m, \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m training_steps ,step)\n\u001b[1;32m     90\u001b[0m     \u001b[39m# Env rollouts\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39m#########################################################\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1205\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m dt \u001b[39m=\u001b[39m cur_t \u001b[39m-\u001b[39m last_print_t\n\u001b[1;32m   1204\u001b[0m \u001b[39mif\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m mininterval \u001b[39mand\u001b[39;00m cur_t \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(n \u001b[39m-\u001b[39;49m last_print_n)\n\u001b[1;32m   1206\u001b[0m     last_print_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_n\n\u001b[1;32m   1207\u001b[0m     last_print_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1256\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrefresh(lock_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlock_args)\n\u001b[1;32m   1257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1258\u001b[0m     \u001b[39m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m     \u001b[39m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m     \u001b[39m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m     \u001b[39m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m     \u001b[39m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval \u001b[39mand\u001b[39;00m dt \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1361\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1360\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[0;32m-> 1361\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay()\n\u001b[1;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[1;32m   1363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1509\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1509\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__str__\u001b[39;49m() \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg)\n\u001b[1;32m   1510\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[1;32m   1511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:350\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_status\u001b[39m(s):\n\u001b[1;32m    349\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 350\u001b[0m     fp_write(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m s \u001b[39m+\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mmax\u001b[39;49m(last_len[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m len_s, \u001b[39m0\u001b[39;49m)))\n\u001b[1;32m    351\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:344\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfp_write\u001b[39m(s):\n\u001b[1;32m    343\u001b[0m     fp\u001b[39m.\u001b[39mwrite(_unicode(s))\n\u001b[0;32m--> 344\u001b[0m     fp_flush()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/utils.py:145\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    147\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/iostream.py:488\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    487\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    489\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    490\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    492\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test on acrobot environment: https://gymnasium.farama.org/environments/classic_control/acrobot/\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "hparams = DEFAULT_HPARAMS\n",
    "agent = train_model(env, ActorCriticNet(env), hparams, path=\"./acrobot\")\n",
    "\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "# generate_video(q, env, \"./acrobot/eval\")\n",
    "(hparams[\"training_steps\"], 1) + env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on acrobot environment: https://gymnasium.farama.org/environments/classic_control/acrobot/\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "hparams = DEFAULT_HPARAMS\n",
    "agent = train_model(env, ActorCriticNet(env), hparams, path=\"./cartpole\")\n",
    "\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "# generate_video(q, env, \"./acrobot/eval\")\n",
    "(hparams[\"training_steps\"], 1) + env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(\"ALE/Pong-v5\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "q = ConvQNet(env)\n",
    "\n",
    "params = DEFAULT_HPARAMS\n",
    "params[\"buffer_size\"] = 50_000\n",
    "params[\"training_steps\"] = 1_000_000\n",
    "\n",
    "q = train_atari_model(env, q, params)\n",
    "\n",
    "generate_video(q, env, \"./pong/eval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
